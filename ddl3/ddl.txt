ddl.go
lex.go
view.go | function.go | trigger.go
table.go
schema.go
t.go
catalog.go

Custom SQL-level filtering when retrieving the catalog is possible! Simply ask the user to provide an sq.Predicate which will get appended to the overall query.

NOTE: if the incoming table's schema is empty, set it to whatever the current default schema is. For postgres this can be found with SELECT current_schema, with mysql this can be found with SELECT database(). For sqlite the default schema is just an empty string.
    This step is important when it comes to diffing the catalogs, because the catalog loaded from the database will contain the schema name while the catalog loaded from the user-provided tables may omit the schema name.
    Maybe just treat the empty schema and the default schemas as the same when diffing the catalog, instead of literally filling in the schema name.

Changeset should somehow accomodate SQLite's migration strategy of constructing a new table, copying the data, dropping the old table and renaming the new table to the old table.

TODO: need to implement RENAME operations as well

TODO: implement Pop(positions ...int) methods that can pop a Schema/Table/Column/Constraint/Index out
This could actually be a generic function lol. Prime use for generics. No need for adding a method on struct that needs Pop.

FUG man functions are giving me so much grief. Have to design something that can potentially be used as a table-valued function (i.e. a struct) but also a function that is defined inside an sql file and the contents are entirely opaque to ddl but somehow must still track overloaded function separately. to handle function diffing properly. And whatever comes out from the database must be representable by this struct.

for SQLite's COPY NEW TABLE -> COPY DATA -> DROP OLD TABLE -> RENAME NEW TABLE flow, we will likely need an extra field in Changeset struct to accomodate DML data copying. The fields will not be from the table themselves, rather (TODO:) ddl.Column should implement sq.Field interface so that they can be used with the query builders instead. This way, given a current table and desired table, we can programatically generate the UPDATE new_table INSERT (from new table) query, by using the Column as sq.Fields directly.

Also, rename operations. Rename table, rename column (rename constraint? rename index?).

t.Trigger(`CREATE TRIGGER actor_last_update_after_update_trg AFTER UPDATE ON actor BEGIN
    UPDATE actor SET last_update = DATETIME('now') WHERE actor_id = NEW.actor_id;
END;`)

gotCatalog, err := NewCatalog("sqlite", ddl.WithDB(db))
if err != nil {
}
wantCatalog, err := NewCatalog("sqlite",
    ddl.WithTables(
        NEW_ACTOR(dialect, ""),
        NEW_CATEGORY(dialect, ""),
        NEW_COUNTRY(dialect, ""),
        NEW_CITY(dialect, ""),
        NEW_ADDRESS(dialect, ""),
        NEW_LANGUAGE(dialect, ""),
        NEW_FILM(dialect, ""),
        NEW_FILM_TEXT(dialect, ""),
        NEW_FILM_ACTOR(dialect, ""),
        NEW_FILM_CATEGORY(dialect, ""),
        NEW_STAFF(dialect, ""),
        NEW_STORE(dialect, ""),
        NEW_CUSTOMER(dialect, ""),
        NEW_INVENTORY(dialect, ""),
        NEW_RENTAL(dialect, ""),
        NEW_PAYMENT(dialect, ""),
        NEW_DUMMY_TABLE(dialect, ""),
        NEW_DUMMY_TABLE_2(dialect, ""),
    ),
    ddl.WithViews(
        NEW_ACTOR_INFO(dialect, ""),
        NEW_CUSTOMER_LIST(dialect, ""),
        NEW_FILM_LIST(dialect, ""),
        NEW_NICER_BUT_SLOWER_FILM_LIST(dialect, ""),
        NEW_SALES_BY_FILM_CATEGORY(dialect, ""),
        NEW_SALES_BY_STORE(dialect, ""),
        NEW_STAFF_LIST(dialect, ""),
    ),
    ddl.WithFunction(`CREATE OR REPLACE FUNCTION last_update_trg()
    RETURNS TRIGGER AS $$ BEGIN
        NEW.last_update = NOW();
        RETURN NEW;
    END; $$ LANGUAGE plpgsql;`),
    ddl.WithFunctionFile(os.DirFS("."), "last_update_trg.sql"),
)
if err != nil {
}
changeset, err := ddl.DiffCatalog(gotCatalog, wantCatalog)
if err != nil {
}
var sql string
for _, cmd := range changeset.Commands() {
    sql, _, _, err = sq.ToSQL(dialect, cmd)
    if err != nil {
    }
    _, err = db.ExecContext(ctx, sql)
    if err != nil {
    }
}
t.Trigger()
for _, cmd := range catalogDiff.Commands(0) {
    sql, err := cmd.ToSQL()
    if err != nil {
    }
    _, err = db.ExecContext(ctx, sql)
    if err != nil {
    }
}

wantCatalog, err := ddl.NewCatalog(dialect, ddl.WithTables(NEW_ACTOR(dialect, "")))
if err != nil {
}
changeset, err := ddl.DiffCatalog(ddl.NewCatalog(dialect), wantCatalog)
if err != nil {
}
buf := &strings.Buffer{}
err = changeset.WriteOut(buf)
if err != nil {
}

create schemas
create tables+columns+constraints (including alter columns)
table dml
create fkey constraints
rename x
drop x
create index
create functions
create views
create triggers

simple:
create schemas
create functions
create table | create or alter columns | create constraints
create index
create functions
create views
create triggers

oh my god if you try to dupe a table with primary keys that are referenced by foreign keys, you need to drop those foreign keys first. then add them back after the rename.

step-by-step:
stage 1: support only creation and column altering
stage 2: support sqlite ghost table/column migrations for otherwise impossible changes (e.g altering constraints)
    sqlite needs less support because fkeys are probably lazily evaluated, so even if you drop old table and rename new table to old table the fkey is probably still valid
stage 3: support postgres/mysql ghost table/column style migration for otherwise impossible changes (e.g. redefining a constraint/index)

for (sqlite/postgres/mysql) when you drop old table -> rename new table to old table, what associations are you breaking?
- I assume fkeys will be violated
- views should be fine because they are evaluated only when called
- triggers and indices will be dropped together with the old table. they will have to be re-added to the new table after it has been renamed.

There is no reason to do the rename+drop flow for postgres/mysql! Because if you are assuming only one writer during migration, then we can do the inefficient and direct alter column methods instead of creating a new column, copying data, renaming and dropping. Same with tables.
Read: DML/DROP/RENAME commands are generated solely for SQLite migrations. Postgres and MySQL migrations will never generate DML/DROP/RENAME.

I can still respect DROPs and RENAMES with postgres/mysql -- they just won't be generated by DiffCatalog. Instead, DROPs and RENAMEs would be part of a multi-step zero-downtime migration process. All written manually by the user. If they don't want to automate it in code they can always write out the commands manually instead and feed it through their migration tool.

To answer: if there is a table_old and table_new, where should I put the constraints for the column? In table(_old) or table_new?

because indexes, functions, views and triggers are created after the rename+delete ops, we don't have to worry about rename breaking the existing table references.
Also, we assume that no one else is doing inserts when the migration is happening so that the table dml can accurately transfer all data from one table to another.

BEGIN;
CREATE table_new
CREATE TRIGGER table_new_trg ON table AFTER INSERT, INSERT INTO table_new
INSERT INTO table_new SELECT * FROM table_old ON CONFLICT DO NOTHING
ALTER TABLE table RENAME TO table_old
ALTER TABLE table_new RENAME TO table
DROP TRIGGER table_new_trg
DROP TABLE table_old
COMMIT;

there's still a bunch of uncertainty of command order when it comes to renaming columns/tables. One issue that sticks out like a sore thumb is the fact that once a column is renamed, -ALL- its constraints and indices should be renamed to reflect the updated table names. What this means is that all the constraints and indices on to-be-renamed tables and columns must be deferred to after rename. ugh
