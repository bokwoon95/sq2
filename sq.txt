sq.go
param.go
fmt.go
predicate.go
rowvalue.go
table_info.go
field_info.go <-> custom_field.go <-> assignment.go
blob_field.go | boolean_field.go | json_field.go | numberfield.go | string_field.go | time_field.go
join_table.go
variadic_query.go
subquery_field.go | cte.go
...
window.go (at very last because nothing depends on it)

tasks:
- documentation site (stitchdocs)
- Select().ForUpdate().ForShare().SkipLocked()
- GroupBy().Cube().Rollup().GroupingSets()
- .Collate() can be called on each field, allowing DDL to no longer require sq.FieldLiteral for collation
    - This means that indexes can also utilize ASC/DESC NULLS FIRST/NULLS LAST without obscuring the field name
- Postgres indexed expressions apparently don't need enclosing brackets. Remove them and check if the tests break.

LoggerDB should no longer be a struct, it should be an interface combining sq.DB and sq.Logger. 
That way the user can craft his own struct like:
type DB struct {
    *sql.DB
    // some other logger state, maybe a redis connection
}
db := &DB{}
db.Query(query, args...) // works
db.Tx() // works
db.GetLogSettings() // works
db.LogQuerySettings() // works
The most amazing thing about embedding *sql.DB in the logger DB is that the
loggerDB struct has access to the database and can run EXPLAIN ANALYZE if the
timeTaken is too slow. Wow!

// LiteralValue will interpolate the value into the query. To make repeatedly
// invoking easier, it helps to define a type alias to to right before using.
type LiteralValue interface{}
type val = sq.LiteralValue
sq.Fieldf("jsonb_build_object({})", []interface{
    val("name"), USER.NAME,
    val("age"), USER.AGE,
    val("score"), USER.SCORE,
})
// jsonb_build_object('name', user.name, 'age', user.age, 'score', user.score)

err := sq.Tx(func(tx *sql.Tx) error {
})

I think each dialect can get its own query builder
- pgsq
- mysq
- sqlt
- sqsv
- ora
- then 'integration tests' would reside wholly inside a separate package and wouldn't be able to contribute to the test coverage of sq, but maybe that's enough
- I'd still need to open an sqlite :memory: database in order to test the *Row and *Column interactions though.
- ohshit but then they'd lose access to the sakila_tables_test.go. keep this on the backburner for now

err := sq.TxContext(ctx, sq.TxRepeatableRead|sq.TxReadOnly, func(tx *sql.Tx) error {
})

// optimistic concurrency control!
tries := 0
err := sq.RetryTxContext(ctx, 0, func(tx *sql.Tx) error {
    version := ...
    rowsAffected, _, err := sq.Exec(Update ... Where version = $version...)
    if rowsAffected < 0 {
        return sq.ErrRetryTx
    }
}, func(txErr error) bool {
    tries++
    time.Sleep(random.Float()*time.Second)
    return errors.Is(txErr, sq.ErrRetryTx) && tries < 10
})

sq.Tx
sq.TxContext
sq.RetryTxContext
sq.Retry
sq.RetryContext

for i := 0; i < 10; i++ {
    db.DoThing()
    if err != nil {
        continue
    }
    break
}

if you want people to be able to log who did what on a table using
logQueryStats, you need to provide the operation and tables affected as well.
At last for inserts and updates and deletes, the table name must be provided.
So that you can dispatch on that and insert the tenantID and the query run into
the corresponding audit table.

now the problem is: even after an insert there is no way of knowing which rows
were just inserted. Unlike the Update and Delete queries, there is no predicate
for Insert queries you can't just convert an Insert query into an equivalent
Select query like you can with an Update query.

I think scrap this idea. The best way is still to do your pre-query and
post-query processing using manually called functions, and you can write more
functions to automate that process (the onus is on you to call those functions
in the first place of course). That way you get full visibility of what data is
going in, what data was returned from INSERT/UPDATE/DELETE ... RETURNING.

sq README needs a FAQ section, just like how Maddy mail server does. I naturally gravitaed towards the FAQ link in order to learn more about maddy, and was able to learn so much it just from reading the FAQ alone.

IMPORTANT: before I do any kind of performance testing, I need to know:
- given a simple SQL query that instantiates a query, runs it and then exits, does the query get stack allocated?
    - I really need this to happen because it's the main reason why I went with queries as struct values instead of struct pointers
    - If I changed the queries to struct pointers instead, would the allocation/performance change?
- assuming a simple query is stack allocated, if I passed the entire query to LogQueryStats would it change to heap allocation?
    - I need to know this because it determines if I want to pass the Query interface to LogQueryStats for analytics purposes.
    - If LogQueryStats gets the Query interface, it can do interesting things like analyzing which tables and columns were most commonly used
    - But if performance takes a hit compared to just passing in the compiled query string and args, then maybe I'll leave it out

pagination_table r good
CREATE TABLE pagination_table (
    table_name TEXT COLLATE "C"
    ,order_by TEXT COLLATE "C"
    ,page_limit INT
    ,page_number INT
    ,starting_id TEXT
);

CREATE UNIQUE INDEX pagination_table_idx ON pagination_table (table_name, order_by, page_limit, page_number) INCLUDE (starting_id);

-- reading
SELECT starting_id FROM pagination_table WHERE (table_name, order_by, page_limit) = ('posts', 'created_at', 10) AND page_number = 3;
SELECT * FROM posts WHERE post_id = $starting_id ORDER BY created_at LIMIT 10;

-- and writing...? how to do the writing?
INSERT INTO posts VALUES (...);
SELECT
    COUNT(*) > 10
FROM
    posts
    JOIN pagination_table ON
        pagination_table.table_name = 'posts'
        AND pagination_table.order_by = 'created_at'
        AND pagination_table.page_limit = 10
