forget about offering users the options to slot in their own predicates.
WithDB's behavior is fixed, take it or leave it. Instead, offer them a way to
write their own CatalogOption function, leveraging the existing IntrospectQuery
and MapColumns/MapTables/etc if necessary.

type PostgresIntrospector struct {
    DB ddl.DB
    IncludeSystemSchemas bool
    IncludeSchemas []string
    ExcludeSchemas []string
    IncludeTables [][2]string
    ExcludeTables [][2]string
    IncludeViews [][2]string
    ExcludeViews [][2]string
}

or should templateData be part of the underlying object instead? This would
allow you to extend existing introspector implementations with your own data.
type ddl.DB = interface{
    QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error)
    ExecContext(ctx context.Context, query string, args ...interface{}) (sql.Result, error)
    PrepareContext(ctx context.Context, query string) (*sql.Stmt, error)
}
type DBIntrospector interface {
    GetDefaultSchema() (defaultSchema string, error)
    GetVersion() (versionString string, versionNum [2]int, error)
    GetExtensions() (extensions [][2]string, supported bool, err error)
    GetSchemas() (schemas []Schema, supported bool, error)
    GetTables() (tbls []Table, supported bool, error)
    GetColumns() (columns []Column, supported bool, error)
    GetConstraints() (constraints []Constraint, supported bool, error)
    GetIndexes() (indexes []Index, supported bool, err error)
    GetTriggers() (triggers []Trigger, supported bool, err error)
    GetViews() (views []View, supported bool, err error)
    GetFunctions() (functions []Function, supported bool, err error)
}
WithDB(ddl.DB)
WithDBIntrospector(db DB, dbIntrospector DBIntrospector)
introspect_sqlite.go
- type SQLiteIntrospector struct
- SQLiteIntrospector is the special kid because before it can introspect columns, it must populate its internal list of tables so that it can run the introspection query over and over for each table.
introspect_postgres.go
- type PostgresIntrospector struct
introspect_mysql.go
- type MySQLIntrospector struct

we *could* decouple the ddl and introspect* packages. That means ddl will no longer be able to choose the introspector for you based on the dialect. Instead the user must always pass in a DB connection and an introspector.

catalog := ddl.NewCatalog(WithDB(db, introspect_sqlite.NewIntrospector()))

catalog := ddl.NewCatalog(ddl.WithDialect(), ddl.WithTables(), ddl.WithDDLViews())

- OR -

catalog := ddl.NewCatalog(dialect, WithDB(db))

catalog := ddl.NewCatalog(dialect, WithDBAndIntrospector(db, dbIntrospector)) // they can write their own option that uses their introspector

catalog := ddl.NewCatalog(dialect, ddl.WithTables(), ddl.WithDDLViews())

ughh I really hate having to repeat the dialect twice first as the dialect string and second as the introspector though.

omfg both postgres and mysql support functions AND stored procedures, so now
I'd have to figure out how that looks from the user-definition side (in Go
files) and the database-definition side (reading from the db).
ALSO: functions may be user-defined aggregate functions or user-defined window
functions. guh.

need a special function just for generating tables + views from the database,
because in those cases you don't want the overhead of introspecting the
functions (which there are quite a lot of in the postgres public schema). In
those cases it will only generate a Catalog with the requisite information for
structs+DDL impl.

ddl.go
column.go | constraint.go | index.go
trigger.go
function.go
table.go
view.go
v.go | t.go
schema.go
catalog.go
migration_commands.go

next up:
bah gawd. extensions. enums.

Also: TYPEs, DOMAINs, ENUMs. aaargh.
I don't need to add these now because the sakila database doesn't use them (I'm
going to strip it out from the postgres schema too). Kick the can down the
road, when the problem is more well-defined.

add dialect modifier
add ignore modifier ignore=postgres
UUIDField can be used with some media table like in devlab

All commands need to exclude the semicolon. The semicolon will be inserted by
(*MigrationCommands).WriteSQL. For MySQL triggers and MySQL
functions/pocedures, (*MigrationCommands).WriteSQL will write in the
appropriate `DELIMITER ;;` command before and after the
triggers/functions/procedures have been defined.

type MigrationOption int

const (
    CreateMissing  MigrationOption = 0b1
    UpdateExisting MigrationOption = 0b10
    DropExtraneous MigrationOption = 0b100
    DropCascade    MigrationOption = 0b1000
)

// pass a tx in explicitly to run everything in a transaction

to opt of of the implicit transactions, sql files must look like xxxxx.no_auto_transaction.up.sql
or maybe instead of hardcoding it, users can pass in a custom function that
takes in the environment and returns true or false whether auto transaction
wrapping should be disabled. a default function can be provided that returns
true of false depending on whether the file ends up no_auto_transaction.up.sql,
but the user can easily substitute one for their own.

// simple use case
ddl.AutoMigrate(dialect, db, ddl.CreateMissing|ddl.UpdateExisting, WithTables(), ...)

// clearing db before use
ddl.AutoMigrate(dialect, tx, ddl.DropExtraneous|ddl.DropCascade)
ddl.DropFunctions(dialect, tx, []Function{})
ddl.AutoMigrate(dialect, tx, ddl.CreateMissing, WithTables(), ...)

DropExtraneous:
- drop tables
- drop columns
- drop constraints
- drop indexes
- drop triggers
- drop views

// logging before executing
gotCatalog, err := NewCatalog(dialect, WithDB(db))
if err != nil {
}
wantCatalog, err := NewCatalog(dialect, WithTables(), ...)
if err != nil {
}
migration, err := ddl.Migrate(ddl.CreateMissing|ddl.UpdateExisting, gotCatalog, wantCatalog)
if err != nil {
}
migration.WriteSQL(os.Stdout)
migration.ExecContext(ctx, db)

// generating migrations
gotCatalog, err := NewCatalog(dialect, WithDB(db))
if err != nil {
}
wantCatalog, err := NewCatalog(dialect, WithTables(), ...)
if err != nil {
}
upMigration, err := Migrate(ddl.CreateMissing|ddl.UpdateExisting, gotCatalog, wantCatalog)
if err != nil {
}
downMigration, err := Migrate(ddl.DropExtraneous|ddl.UpdateExisting, wantCatalog, gotCatalog)
if err != nil {
}
upMigration.WriteSQL(f1)
downMigration.WriteSQL(f2)

if the user eventually wants to expose the sq.Query produced by a DDLView, I
can add a public .Query field to V. Make a new constructor called NewV(dialect
string, view *View, wantColumns []string) to keep the fields private but allow
the user to inject them. Of course all this can be added later, only when
required. I don't have to add it in now.

dont bother with diffing catalogs to migrations at first:
func (c Catalog) GenerateDDL(w io.Writer) error
func (c Catalog) GenerateStructs(w io.Writer) error
GenerateDDL will append commands directly into a MigrationCommands struct,
bypassing the CatalogMigration struct, because we have no need for it.

to clean a database (for example to reset its state to zero), all you have to
do is to loop the gotCatalog Catalog struct, for each table and view and
function you simply convert it to the corresponding
DropTableCommand/DropViewCommand/DropFunctionCommand and append it into the
MigrationCommands struct. You can then either execute the MigrationCommands
directly or write it out to a file, the choice is yours.

v.AsQuery()
v.SQL() // used if you have the view literal
This means the View struct must now store the full view definition, and not
just the select query part. It also means we have to omit the IsMaterialized
part of the struct. Everything compiles down to an SQL string.

automigrate ops:
- add schema
- add table
- add column
- alter column nullable, default
- change index columns
generate migration ops:
- all automigrate ops

trigger_migration.go
function_migration.go
view_migration.go
table_migration.go
schema_migration.go
catalog_migration.go

func MigrateCatalog(gotCatalog, wantCatalog Catalog) (CatalogMigration, error)
func MigrateSchema(gotCatalog Catalog, wantSchema Schema) (SchemaMigration, error)
func MigrateTable(dialect string, gotSchema Schema, wantTable Table) (TableMigration, error)
func MigrateColumn(dialect string, gotTable Table, wantColumn Column) (ColumnMigration, error)

func (c Catalog) MigrateCatalog(wantCatalog Catalog) (CatalogMigration, error)
func (c Catalog) MigrateSchema(wantSchema Schema) (SchemaMigration, error)
func (c Catalog) MigrateTable(wantTable Table) (TableMigration, error)
func (c Catalog) MigrateColumn(wantColumn Column) (ColumnMigration, error)
// the below 2 can detect identity, hence we can generate alter migrations. but
non-online migrations will not be possible.
func (c Catalog) MigrateConstraint(wantConstraint Column) (ConstraintMigration, error)
func (c Catalog) MigrateIndex(wantIndex Index) (IndexMigration, error)
// all the migrations below should be create-only i.e. no alteration is possible
func (c Catalog) MigrateView(wantView View) (ViewMigration, error)
func (c Catalog) MigrateFunction(wantFunction Function) (FunctionMigration, error)
func (c Catalog) MigrateTrigger(wantTrigger Trigger) (TriggerMigration, error)

do we want to implement support for dropping unused tables/columns/constraints/indexes/views/functions/triggers? something like
func (c Catalog) WantCatalog(wantCatalog Catalog) (CatalogMigration, error) will generate the create/alter commands. whereas
func (c Catalog) GotCatalog(gotCatalog Catalog) (CatalogMigration, error) will generate the drop commands.

generating online migrations for functions and views is a huge jar of worms.
because functions can reference other functions and views can reference other
views, it is not sufficient to simply create a new function/view, atomically
rename current to old and new to current, then drop old. Because even if you
rename current to old, there may be other functions that are still referencing
old. You must work out the depedency graph and rebuild all depedencies, which
sucks major ass. Therefore, alteration for functions and views should not be
supported at all.
generating non-online function and view is impossible, because we cannot detect
function/view identity.

generating online migrations for altering indexes is also not possible, but for
different reasons. You cannot build a new index alongside the old index,
because they might not be valid at the same time. For example, the reason for
changing an index might be because a column is removed. So keeping the old
index around may be impossible because a referenced column no longer exists.
basically don't supprt altering indexes as well.
generating non-online index migration is possible, because we can detect index
identity.

triggers can't be altered online safely because two triggers living
side-by-side may result in incorrect behaviour.
generating non-online trigger migrations is also not possible because we cannot
detect trigger identity.

func (m CatalogMigration) Commands() MigrationCommands
type MigrationCommands struct {
	Dialect               string
	SchemaCommands        []Command
	FunctionCommands      []Command
	TableCommands         []Command
	ViewCommands          []Command
	TableFunctionCommands []Command
	TriggerCommands       []Command
	DualWriteTriggers     []Command
	BackfillQueries       []sq.Query
	GhostTableCommands    []Command
	ForeignKeyCommands    []Command
	RenameCommands        []Command
	DropCommands          []Command
}

Idea: MigrateCatalog() should generate all of the drop (unused) X commands as well. AutoMigrate will simply ignore the drop queries.
    but what about the scheduled drop (old <-> new) queries, like for migrating tables in sqlite?
